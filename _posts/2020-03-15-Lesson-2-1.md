---
title: "Lesson 2-1"
date: 2020-03-15 01:58:28 -0400
categories: Pytorch
use_math: true
comments: true
---

# Introduction to Neural Networks

## 1. Perceptrons

### 1.1. Basics

**Classification Problem** : predicting label ($y$) by features($x_i$)
Notations : 
* **Feature** $x$ = $\begin{bmatrix} x_1,x_2,...,x_n\end{bmatrix}$
* **Weights** $W$ = $\begin{bmatrix} w_1,w_2,...,w_n\end{bmatrix}$
* **Bias** $b$
* **Label** $y\in \{0,1\}$ 
* **Prediction** $\hat y\in\{0,1\}$

이 때 **Score**는
$$\sum_{i=1}^nw_ix_i+b=Wx+b$$
와 같은 linear function이고, prediction은
$$\hat y={0~~~(\text{if}~~~Wx+b\geq 0)
\brace
1~~~(\text{if}~~~Wx+b<0) }$$
와 같은 step fuction이다.

이를 **Perceptron**으로 implementation할 수 있다. Perceptron은 여러개의 input을 weight을 곱하여 받아 더한 후, **activation function** 을 통해 결과값을 return한다. Discrete perceptron을 구성하는 방법은 다음과 같이 두 가지가 있으며, 후자가 더 선호된다.
1. **Bias as a Node**
	```mermaid
			graph LR
			bias((b ))
			x1--w1-->bias
			x2--w2-->bias
			...
			xn--wn-->bias	
			bias-->s((step func))
			s --> prd("output (0/1)")
	```
	
2. **Bias as an Input**
	```mermaid
		graph LR
		sum(("&sum;"))
		x1--w1-->sum
		x2--w2-->sum
		...
		xn--w3-->sum
		1--b-->sum
		sum-->s((step func))
		s --> prd("output (0/1)")
	```
### 1.2. Perceptron Network
Perceptron을 매우 많이 연결시켜(concatenating) **Neural Network**를 구성할 수 있다. 단순히 몇 개만 연결시켜도 간단한 기능을 수행할 수 있는데, 예를 들면 perceptron들로 logical gate를 만들어낼 수 있다.
1. AND / OR : linear boundary 하나로 구현 가능.
	e.g.) AND : $x_1+x_2-1.5$,
	OR : increase $w$ or decrease the magnitude of $b$
2. NOT : vertical boundary, $-x_1+0.5$
3. XOR : gate의 연결로 구성 가능
	```mermaid
	graph LR
	x1((x1)); x2((x2))
	x1 --> AND; x2 --> AND; x1 --> OR ; x2 --> OR
	AND --> NOT; NOT --> new[AND]; OR --> new
	new --> XOR(XOR)
	```

### 1.3. Perception Algorithms

#### 1.3.1. Linear

목표는 적절한 $w_i, b$값을 고르는 것. 처음에 임의의 $w_i, b$에서 시작한 다음, prediction과 비교하여 **learning rate**($\alpha$)만큼 feedback을 하는 iteration을 진행한다. Learning rate와 epoch (# of iterations)는 적절한 값으로 선택
```
W, b = random values
for every misclassified point (x1,x2,...,xk) {
		if prediction == 0 {	// label == 1
			W = W + alpha * xi
			b = b + alpha
		}
		if prediction == 1 {	// label == 0
			W = W - alpha * xi
			b = b - alpha
		}
	}
// 여기까지가 1 iteration
```
아래는 Quiz에서 python으로 구현한 코드
```python
# X[m][n] : features
#	m : # of training set
#	n : # of features
# W[n] : weights
# y[n] : labels
def perceptronStep(X, y, W, b, learn_rate = 0.01):
    for i in range(len(X)):
        y_hat = prediction(X[i],W,b)
        if y[i]-y_hat == 1:
            W[0] += X[i][0]*learn_rate
            W[1] += X[i][1]*learn_rate
            b += learn_rate
        elif y[i]-y_hat == -1:
            W[0] -= X[i][0]*learn_rate
            W[1] -= X[i][1]*learn_rate
            b -= learn_rate
    return W, b
```

<br>

#### 1.3.2 Non-linear
비선형 모델의 경우 위와 같은 간단한 trick으로 해결할 수는 없다. 따라서, perceptron algorithm을 재정의하여 non-linear boundary를 형성할 수 있게 해야 한다.

<br>

## 2. Error Function

### 2.1. Sigmoid Function : probability output

적합한 prediction (*solution*)을 찾기 위해서는 **Error Fuction**의 도입이 필요한데, error function에서 *gradient descent*를 적용하여 최적해를 찾으려면 error function이 *log loss error function*과 같이 **continuous**해야 한다.
따라서 이를 수행하는 perceptron도 continuous한 결과값을 return해야 하는데, 이는 perceptron의 activation function을 **Sigmoid Function**으로 수정함으로써 해결할 수 있다. 다음은 sigmoid function 중 하나인 **Logistic Function**이다.
>$$
\sigma(x)=\frac{1}{1+e^{-x}}
$$

![sigmoid](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/640px-Logistic-curve.svg.png)

Perceptron을 sigmoid로 구성하면 결과값이 0과 1사이의 실수가 된다. 즉, 결과값이 0에 가까울 수록 0이 될 "*확률*"이 높고, 1에 가까울수록 1이 될 "*확률*"이 높은 것이다.

### 2.2. Softmax : for multi-class

Classification Problem의 class가 3개 이상일 때, 다음과 같은 **Softmax Function**을 이용한다. Class가 $n$개일 때, linear function인 score $z_1,...,z_n$에 대해
>$$P(\text{class}=i)=\frac{e^{z_i}}{e^{z_1} +e^{z_2}+...+e^{z_n}}$$

이 때  class가 2개인 softmax function은 logistic function과 같음을 쉽게 확인할 수 있다.

```python
import numpy as np

def softmax(L):
    expL = np.exp(L)		# list 한꺼번에 exp 계산
    sumExpL = sum(expL)		# list를 sum up
    result = []
    for i in expL:
        result.append(i*1.0/sumExpL)
    return result
    
    # Note: The function np.divide can also be used here,  as   follows:
    # def softmax(L):
    #     expL = np.exp(L)
    #     return np.divide (expL, expL.sum())
```

### 2.3. One-Hot Encoding
이제 multi-class classification problem이 *numerical data*가 아닌 *categorical data*인 경우를 생각해 보자. 다중 class에 대해, class마다 1,2,3,...과 같이 숫자를 부여한 후 데이터로 이용하면, discrete data인 class(category)가 continuous한 속성을 띠는 것 처럼 보일 수 있다. 즉, class 1과 class 5의 중간이 class 3이라던가...

이런 문제를 발생시키지 않기 위해서 **One-Hot Encoding** 을 사용한다. 이는 하나의 class당 하나의 variable을 부여하고, 데이터 유무를 0과 1로만 나타낸다. 즉, class가 $n$개인 categorical data는 각각 `[0,0,...,1,...,0]`과 같은 $n$차원 벡터로 나타내어진다.

<br>

### 2.4 Cross-Entropy
이제 prediction을 확률로 얻었으니, prediction 결과 확률이 가장 높은 model을 선택하기만 하면 된다. 그러나 전체 확률은 각 point의 확률의 곱으로 표현되므로, 이를 간단히 나타내기 위해 log를 도입하여 **Cross-Entropy**라는 새로운 값을 정의한다.
> **Cross-Entropy (CE) :** 
> $$
> CE=-\sum_{i=1}^m (y_i\ln p_i +(1-y_i)\ln (1-p_i))
> $$
> $y_i \in \{0,1\}$ : label
> 
> $p_i \in [0,1]$ : probability prediction (= $\hat y_i$ or $1-\hat y_i$)

```python
import numpy as np

def cross_entropy(Y, P):
    Y = np.float_(Y)
    P = np.float_(P)
    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))
    # numpy에서는 array간 element-wise product는 *로 계산할 수 있다.
```
즉 *maximum likelihood problem*은 *minimizing cross-entropy*와 같으며, $CE$를 최소화하는 model을 찾으면 된다.

Multi-class의 경우 CE는 다음과 같이 계산할 수 있다.

|| point 1 | point 2 | ... | point m |
|:---:|:---:|:---:|:---:|:---:|
|class 1|$p_{11}$|$p_{12}$|...|$p_{1m}$|
|class 2|$p_{21}$|$p_{22}$|...|$p_{2m}$|
|||...|||
|class n|$p_{n1}$|$p_{n2}$|...|$p_{nm}$|
> $$
    CE = -\sum_{i=1}^n \sum_{j=1}^n y_{ij}\ln p_{ij}
> $$
> $x_{ij}$ : "is $j^{th}$ point $i^{th}$ class?"
